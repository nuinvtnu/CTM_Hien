{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"x7LBfh0uLpup","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1115ab19-124c-43e0-bf19-93d91d0fbb6f","executionInfo":{"status":"ok","timestamp":1732868269131,"user_tz":-420,"elapsed":22011,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"HOc-mgrK5B_6","executionInfo":{"status":"ok","timestamp":1732868277874,"user_tz":-420,"elapsed":8747,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import pickle as cPickle\n","import pandas as pd\n","from keras.callbacks import EarlyStopping\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.layers import LSTM, Dense, TimeDistributed, Bidirectional\n","import sklearn.metrics\n","from sklearn.metrics import confusion_matrix\n","\n","from sklearn.model_selection import KFold\n","from keras.models import Sequential\n","from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n","from keras.layers import concatenate\n","from tensorflow.keras import Model\n","from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional\n","from keras import models"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Mt34EtLoLpup","executionInfo":{"status":"ok","timestamp":1732868277875,"user_tz":-420,"elapsed":7,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}}},"outputs":[],"source":["path_data = \"/content/drive/MyDrive/CTM_Hien-main/Data/\"\n","path_result = \"/content/drive/MyDrive/CTM_Hien-main/Result/\"\n","path_model = \"/content/drive/MyDrive/CTM_Hien-main/Model/\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CUUYvlp8Lpup","executionInfo":{"status":"ok","timestamp":1732868277875,"user_tz":-420,"elapsed":6,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}}},"outputs":[],"source":["def twoTupleDic3():\n","    AA_list_sort = ['G','A','V','L','I','M','P','F','W','S','T','N','Q','Y','C','K','R','H','D','E','X']\n","\n","    AA_dict = {}\n","    numm = 1\n","    for i in AA_list_sort:\n","        for j in AA_list_sort:\n","          for jj in AA_list_sort:\n","             AA_dict[i+j+jj] = numm\n","             numm += 1\n","    return AA_dict\n","def twoTupleDic2():\n","    AA_list_sort = ['G','A','V','L','I','M','P','F','W','S','T','N','Q','Y','C','K','R','H','D','E','X']\n","\n","    AA_dict = {}\n","    numm = 1\n","    for i in AA_list_sort:\n","        for j in AA_list_sort:\n","          AA_dict[i+j] = numm\n","          numm += 1\n","    return AA_dict\n","\n","def twoTupleDic1():\n","    AA_list_sort = ['G','A','V','L','I','M','P','F','W','S','T','N','Q','Y','C','K','R','H','D','E','X']\n","\n","    AA_dict = {}\n","    numm = 1\n","    for i in AA_list_sort:\n","        AA_dict[i] = numm\n","        numm += 1\n","    return AA_dict\n","def ProSentence(pro, K):\n","\tsentence = \"\"\n","\tlength = len(pro)\n","\tfor i in range(length - K + 1):\n","\t\tsentence += pro[i: i + K] + \" \"\n","    #delete extra space\n","\tsentence = sentence[0 : len(sentence) - 1]\n","\treturn sentence\n","k =1#1-gram\n","word_index1 = twoTupleDic1()\n","vocab_size = len(word_index1)\n"]},{"cell_type":"code","source":["# Tải dữ liệu train model\n","import pandas as pd\n","file_train = \"train_data_3speaces_31.csv\"\n","df_train=pd.read_csv(path_data + file_train, delimiter= ',')\n","\n","texts_train =[] #PTMsequend kmer\n","for i in df_train['Sequence']:\n","  temp = ProSentence(i,k)\n","  texts_train.append(temp)\n","df_train['k_mer'] =texts_train\n","train_sequences = []\n","for each in texts_train:\n","    each_index_list = []\n","    each = each.split(' ')\n","    for i in each:\n","        each_index_list.append(word_index1[i])\n","    train_sequences.append(each_index_list)\n","# Tokenizer train data input Word2vec\n","data_token = []\n","for i in df_train['k_mer']:\n","   data_token.append(i.split())\n","\n","MAX_SEQUENCE_LENGTH= len(data_token[1])\n","Xtrain = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","ytrain = np.array(df_train['Label'])\n","ytrain = np.array(ytrain)\n","# perform one-hot encoding on the labels\n","lb = LabelBinarizer()\n","ytrain = lb.fit_transform(ytrain)\n","ytrain= to_categorical(ytrain)\n","ytrain.shape\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2vrPBaap1Q7","executionInfo":{"status":"ok","timestamp":1732868279106,"user_tz":-420,"elapsed":1236,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}},"outputId":"06e8d9be-2bbe-4aa0-de00-6951ecfac573"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6746, 2)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["**Define model**"],"metadata":{"id":"Hud3BNBGkcH_"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"o92YRtPh5Knd","executionInfo":{"status":"ok","timestamp":1732868279107,"user_tz":-420,"elapsed":6,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}}},"outputs":[],"source":["class Distiller(keras.Model):\n","    def __init__(self, student, teacher):\n","        super().__init__()\n","        self.teacher = teacher\n","        self.student = student\n","\n","    def compile(\n","        self,\n","        optimizer,\n","        metrics,\n","        student_loss_fn,\n","        distillation_loss_fn,\n","        alpha=0.1,\n","        temperature=3,\n","    ):\n","        \"\"\" Configure the distiller.\n","\n","        Args:\n","            optimizer: Keras optimizer for the student weights\n","            metrics: Keras metrics for evaluation\n","            student_loss_fn: Loss function of difference between student\n","                predictions and ground-truth\n","            distillation_loss_fn: Loss function of difference between soft\n","                student predictions and soft teacher predictions\n","            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n","            temperature: Temperature for softening probability distributions.\n","                Larger temperature gives softer distributions.\n","        \"\"\"\n","        super().compile(optimizer=optimizer, metrics=metrics)\n","        self.student_loss_fn = student_loss_fn\n","        self.distillation_loss_fn = distillation_loss_fn\n","        self.alpha = alpha\n","        self.temperature = temperature\n","\n","    def train_step(self, data):\n","        # Unpack data\n","        x, y = data\n","\n","        # Forward pass of teacher\n","        teacher_predictions = self.teacher(x, training=False)\n","\n","        with tf.GradientTape() as tape:\n","            # Forward pass of student\n","            student_predictions = self.student(x, training=True)\n","\n","            # Compute losses\n","            student_loss = self.student_loss_fn(y, student_predictions)\n","\n","            # Compute scaled distillation loss from https://arxiv.org/abs/1503.02531\n","            # The magnitudes of the gradients produced by the soft targets scale\n","            # as 1/T^2, multiply them by T^2 when using both hard and soft targets.\n","            distillation_loss = (\n","                self.distillation_loss_fn(\n","                    tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n","                    tf.nn.softmax(student_predictions / self.temperature, axis=1),\n","                )\n","                * self.temperature**2\n","            )\n","\n","            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n","\n","        # Compute gradients\n","        trainable_vars = self.student.trainable_variables\n","        gradients = tape.gradient(loss, trainable_vars)\n","\n","        # Update weights\n","        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","        # Update the metrics configured in `compile()`.\n","        self.compiled_metrics.update_state(y, student_predictions)\n","\n","        # Return a dict of performance\n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update(\n","            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n","        )\n","        return results\n","\n","    def test_step(self, data):\n","        # Unpack the data\n","        x, y = data\n","\n","        # Compute predictions\n","        y_prediction = self.student(x, training=False)\n","\n","        # Calculate the loss\n","        student_loss = self.student_loss_fn(y, y_prediction)\n","\n","        # Update the metrics.\n","        self.compiled_metrics.update_state(y, y_prediction)\n","\n","        # Return a dict of performance\n","        results = {m.name: m.result() for m in self.metrics}\n","        results.update({\"student_loss\": student_loss})\n","        return results\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"O336skmC67je","executionInfo":{"status":"ok","timestamp":1732868279108,"user_tz":-420,"elapsed":6,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}}},"outputs":[],"source":["checkpoint = EarlyStopping(monitor='val_loss',\n","            min_delta=0,\n","            patience=3,\n","            verbose=1, mode='auto')\n","result_test =path_result +\"Result_KD.txt\"\n","num_folds = 5\n","TIME_STEPS = 33\n","INPUT_SIZE = 300\n"]},{"cell_type":"code","source":["#  Cross validation Knowlege distillation model\n","kf = KFold(n_splits=num_folds, shuffle=True)\n","\n","X = Xtrain\n","Y = ytrain\n","model_teacher = models.load_model(path_model + 'model_teacher.h5')\n","fold_idx =1\n","for train_index, test_index in kf.split(X,Y):\n","    X_train, X_test =X[train_index], X[test_index]\n","    Y_train, Y_test = Y[train_index], Y[test_index]\n","# Create the student\n","    print(\"Start train Fold \", fold_idx)\n","    student = keras.Sequential(\n","    [\n","        layers.Embedding(vocab_size+1, 300, input_length=MAX_SEQUENCE_LENGTH,trainable=True),\n","        #layers.dropout(0.2),\n","        layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n","        layers.Dropout(0.2),\n","        #layers.LSTM(300, dropout=0.2, recurrent_dropout=0.2),\n","        layers.Flatten(),\n","        layers.Dense(128),\n","        layers.Dropout(0.2),\n","        layers.Dense(2),\n","        layers.Activation('softmax'),\n","    ],\n","    name=\"student\",\n","    )\n","    student_scratch = keras.models.clone_model(student)\n","    distiller = Distiller(student=student, teacher= model_teacher)\n","    distiller.compile(\n","          student_loss_fn= keras.losses.CategoricalCrossentropy(from_logits=True),\n","          #student_loss_fn= keras.losses.BinaryCrossentropy(from_logits=True),\n","          optimizer=keras.optimizers.Adam(learning_rate = 0.0001),\n","          metrics=[\n","                  \"categorical_accuracy\",\n","                  \"AUC\",\n","                  ], # , f1\n","          distillation_loss_fn=keras.losses.KLDivergence(),\n","          alpha=0.1,\n","          temperature=10\n","    )\n","    history_KD = distiller.fit(X_train, Y_train, batch_size=16, validation_data =(X_test, Y_test), epochs=100, callbacks=[checkpoint],verbose=1)\n","    result2 = distiller.evaluate(X_test, Y_test)#, validation_data=(X_test,Y_test), shuffle = True,callbacks=[checkpoint],verbose=1)\n","    f = open(result_test, 'a+', encoding='UTF-8')\n","    f.write(\"\\n KD_BiLSTM fold: \\n\")\n","    f.write(str(fold_idx))\n","    s = str(result2)\n","    f.write(s)\n","    f.close()\n","\n","\n","    ypred = student.predict(X_test)\n","    ypred =np.argmax(ypred,axis =1)\n","\n","    ytest_true = np.argmax(Y_test,axis =1)\n","\n","    result22 = confusion_matrix(ytest_true,ypred)\n","    print(result22)\n","    f = open(result_test, 'a+', encoding='UTF-8')\n","    f.write(\"\\n Fold Confusion_matric: \\n \")\n","    s = str(result22)\n","    f.write(s)\n","    f.close()\n","    fold_idx = fold_idx +1"],"metadata":{"id":"JjvLbtJWtx-m","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e92192c0-9283-4179-f07e-f2c313bbf8e0","executionInfo":{"status":"ok","timestamp":1732868906478,"user_tz":-420,"elapsed":30312,"user":{"displayName":"CÙ THỊ MAI HIÊN","userId":"13472880489405816895"}}},"execution_count":8,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Start train Fold  1\n","Epoch 1/100\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:593: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:592: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n","```\n","for metric in self.metrics:\n","    metric.update_state(y, y_pred)\n","```\n","\n","  return self._compiled_metrics_update_state(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - AUC: 0.6439 - categorical_accuracy: 0.6051 - distillation_loss: 0.0098 - loss: 0.5000 - student_loss: 0.6260 - val_AUC: 0.7846 - val_categorical_accuracy: 0.7111 - val_loss: 0.5000 - val_student_loss: 0.5050\n","Epoch 2/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 81ms/step - AUC: 0.7769 - categorical_accuracy: 0.7098 - distillation_loss: 0.0022 - loss: 0.5000 - student_loss: 0.5737 - val_AUC: 0.7945 - val_categorical_accuracy: 0.7252 - val_loss: 0.5000 - val_student_loss: 0.4206\n","Epoch 3/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 75ms/step - AUC: 0.7831 - categorical_accuracy: 0.7232 - distillation_loss: 0.0016 - loss: 0.5000 - student_loss: 0.5645 - val_AUC: 0.7953 - val_categorical_accuracy: 0.7244 - val_loss: 0.5000 - val_student_loss: 0.4185\n","Epoch 4/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 73ms/step - AUC: 0.7814 - categorical_accuracy: 0.7202 - distillation_loss: 0.0014 - loss: 0.5000 - student_loss: 0.5638 - val_AUC: 0.8076 - val_categorical_accuracy: 0.7407 - val_loss: 0.5000 - val_student_loss: 0.4633\n","Epoch 4: early stopping\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - AUC: 0.8387 - categorical_accuracy: 0.7843 - loss: 0.5000 - student_loss: 0.5371\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n","[[485 188]\n"," [162 515]]\n","Start train Fold  2\n","Epoch 1/100\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:593: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:592: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n","```\n","for metric in self.metrics:\n","    metric.update_state(y, y_pred)\n","```\n","\n","  return self._compiled_metrics_update_state(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 82ms/step - AUC: 0.6446 - categorical_accuracy: 0.6091 - distillation_loss: 0.0094 - loss: 0.5000 - student_loss: 0.6185 - val_AUC: 0.7910 - val_categorical_accuracy: 0.7228 - val_loss: 0.5000 - val_student_loss: 0.7935\n","Epoch 2/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 77ms/step - AUC: 0.7703 - categorical_accuracy: 0.6981 - distillation_loss: 0.0020 - loss: 0.5000 - student_loss: 0.5742 - val_AUC: 0.8011 - val_categorical_accuracy: 0.7391 - val_loss: 0.5000 - val_student_loss: 0.8032\n","Epoch 3/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 72ms/step - AUC: 0.7812 - categorical_accuracy: 0.7191 - distillation_loss: 0.0015 - loss: 0.5000 - student_loss: 0.5654 - val_AUC: 0.8013 - val_categorical_accuracy: 0.7487 - val_loss: 0.5000 - val_student_loss: 0.6987\n","Epoch 3: early stopping\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - AUC: 0.7982 - categorical_accuracy: 0.7523 - loss: 0.5000 - student_loss: 0.5530\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n","[[516 144]\n"," [195 494]]\n","Start train Fold  3\n","Epoch 1/100\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:593: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:592: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n","```\n","for metric in self.metrics:\n","    metric.update_state(y, y_pred)\n","```\n","\n","  return self._compiled_metrics_update_state(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 77ms/step - AUC: 0.6408 - categorical_accuracy: 0.5931 - distillation_loss: 0.0094 - loss: 0.5000 - student_loss: 0.6164 - val_AUC: 0.7865 - val_categorical_accuracy: 0.7168 - val_loss: 0.5000 - val_student_loss: 0.5075\n","Epoch 2/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 68ms/step - AUC: 0.7763 - categorical_accuracy: 0.7150 - distillation_loss: 0.0019 - loss: 0.5000 - student_loss: 0.5717 - val_AUC: 0.7896 - val_categorical_accuracy: 0.7205 - val_loss: 0.5000 - val_student_loss: 0.5534\n","Epoch 3/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 77ms/step - AUC: 0.7790 - categorical_accuracy: 0.7163 - distillation_loss: 0.0016 - loss: 0.5000 - student_loss: 0.5616 - val_AUC: 0.7949 - val_categorical_accuracy: 0.7198 - val_loss: 0.5000 - val_student_loss: 0.5155\n","Epoch 3: early stopping\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - AUC: 0.8089 - categorical_accuracy: 0.7373 - loss: 0.5000 - student_loss: 0.5562\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n","[[511 187]\n"," [191 460]]\n","Start train Fold  4\n","Epoch 1/100\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:593: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:592: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n","```\n","for metric in self.metrics:\n","    metric.update_state(y, y_pred)\n","```\n","\n","  return self._compiled_metrics_update_state(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 98ms/step - AUC: 0.6276 - categorical_accuracy: 0.5798 - distillation_loss: 0.0096 - loss: 0.5000 - student_loss: 0.6168 - val_AUC: 0.7590 - val_categorical_accuracy: 0.6998 - val_loss: 0.5000 - val_student_loss: 0.7084\n","Epoch 2/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 78ms/step - AUC: 0.7888 - categorical_accuracy: 0.7208 - distillation_loss: 0.0022 - loss: 0.5000 - student_loss: 0.5658 - val_AUC: 0.7684 - val_categorical_accuracy: 0.7146 - val_loss: 0.5000 - val_student_loss: 0.7773\n","Epoch 3/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - AUC: 0.7955 - categorical_accuracy: 0.7344 - distillation_loss: 0.0016 - loss: 0.5000 - student_loss: 0.5569 - val_AUC: 0.7762 - val_categorical_accuracy: 0.7153 - val_loss: 0.5000 - val_student_loss: 0.7314\n","Epoch 3: early stopping\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - AUC: 0.7869 - categorical_accuracy: 0.7297 - loss: 0.5000 - student_loss: 0.5812\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","[[496 174]\n"," [210 469]]\n","Start train Fold  5\n","Epoch 1/100\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:593: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n","/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:592: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n","```\n","for metric in self.metrics:\n","    metric.update_state(y, y_pred)\n","```\n","\n","  return self._compiled_metrics_update_state(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 77ms/step - AUC: 0.6306 - categorical_accuracy: 0.5819 - distillation_loss: 0.0094 - loss: 0.5000 - student_loss: 0.6149 - val_AUC: 0.7814 - val_categorical_accuracy: 0.7064 - val_loss: 0.5000 - val_student_loss: 0.3990\n","Epoch 2/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 77ms/step - AUC: 0.7793 - categorical_accuracy: 0.7160 - distillation_loss: 0.0020 - loss: 0.5000 - student_loss: 0.5680 - val_AUC: 0.7896 - val_categorical_accuracy: 0.7317 - val_loss: 0.5000 - val_student_loss: 0.4569\n","Epoch 3/100\n","\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 79ms/step - AUC: 0.7992 - categorical_accuracy: 0.7348 - distillation_loss: 0.0015 - loss: 0.5000 - student_loss: 0.5627 - val_AUC: 0.7930 - val_categorical_accuracy: 0.7309 - val_loss: 0.5000 - val_student_loss: 0.3989\n","Epoch 3: early stopping\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - AUC: 0.8215 - categorical_accuracy: 0.7530 - loss: 0.5000 - student_loss: 0.5536\n","\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n","[[490 182]\n"," [181 496]]\n"]}]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}